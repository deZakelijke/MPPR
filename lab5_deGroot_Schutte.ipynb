{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Principles in Pattern Recognition (2016/2017)\n",
    "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bA}{\\mathbf{A}}$\n",
    "$\\newcommand{\\bQ}{\\mathbf{Q}}$\n",
    "$\\newcommand{\\bR}{\\mathbf{R}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5\n",
    "\n",
    "In the computer labs we will work with the Python programming language within a Jupyter notebook. Each week a new notebook is made available that contains the exercises that are to be handed-in. \n",
    "\n",
    "* You are expected to work in pairs\n",
    "* Only one of each pair has to submit on blackboard. Make sure that you add the student ID of your partner in the submission comments.\n",
    "* The main notebook file you submit should read \"Lab[number]_[last name 1]_[last name 2].ipynb\", for example \"Lab2_Bongers_Versteeg.ipynb\". \n",
    "* Please make sure your code will run without problems!\n",
    "\n",
    "Feel free ask any questions during the computer lab sessions, or email the TA, Elise (e.e.vanderpol@uva.nl).\n",
    "\n",
    "\n",
    "**The due date for the labs is Friday, Oct 27 at 23:59**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will work on one final project, instead of the step-by-step exercises of previous labs.\n",
    "\n",
    "## 1. Final project: Overfitting\n",
    "**[100 points]** Create a project about overfitting in the remainder of this notebook using markdown cells for equations and comments and code cells for code. Make sure to touch upon the following topics:\n",
    "1. Use the wine data set to show what *overfitting* is in terms of a regression problem. (see: white_data.npy, white_targets.npy, red_data.npy, red_targets.npy)\n",
    "2. Discuss how low and high *bias* and *variance* come into play here using figure(s), and write down what *model complexity* has to do with it.\n",
    "3. One way to deal with your overfitted data in a frequentist setting is regularized regression. Use your pick of regularized regression here and apply a cross-validation scheme to determine the regularization parameter $\\lambda$. \n",
    "4. Finally, shortly explain the Bayesian point-of-view on what you have done and how this would prevent overfitting. How could you use the Bayesian method to select the best model for your data? Contrast between model averaging and model selection and use the latter to select a good model.\n",
    "\n",
    "For more background information, refer to Bishop 1.1, 1.3, 1.5, 3.1.4, 3.2, 3.4!\n",
    " \n",
    "Notes on implementation:\n",
    "* Make sure that your hand-in is self-contained, understandable to read from start to end with an introduction about overfitting and overall conclusion or outlook.\n",
    "* This time we emphasize code cleanness and will allocate **[20 points]** to the readability of your code and graphical output.\n",
    "* Use your own implementations instead of standard Python machine-learning tools, like `sk-learn`. More standard modules like `numpy` are allowed as always.\n",
    "* As always: make sure you submit all included data and files necessary to run your notebook out-of-the-box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We converted the data files to python 3\n",
    "def read_input_data():\n",
    "    white_feat = load('white_data_beter.npy')\n",
    "    white_tar = load('white_targets_beter.npy')\n",
    "    red_feat = load('red_data_beter.npy')\n",
    "    red_tar = load('red_targets_beter.npy')\n",
    "    return white_feat, white_tar, red_feat, red_tar\n",
    "\n",
    "\n",
    "def design_matrix(n, white_feat, red_feat):\n",
    "    white_size = shape(white_feat)\n",
    "    red_size = shape(red_feat)\n",
    "    feat_size = 1 + n*white_size[1]\n",
    "    X = ones((white_size[0]+red_size[0], feat_size))\n",
    "    if n==1:\n",
    "        X[:white_size[0], 1:] = white_feat\n",
    "        X[white_size[0]:, 1:] = red_feat\n",
    "        return X\n",
    "    elif n== 2:\n",
    "        X[:white_size[0], 1:white_size[1]+1] = white_feat\n",
    "        X[white_size[0]:, 1:white_size[1]+1] = red_feat\n",
    "        X[:white_size[0], white_size[1]+1:] = square(white_feat)\n",
    "        X[white_size[0]:, white_size[1]+1:] = square(red_feat)\n",
    "        return X\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "def train_test_split(X, target, ratio):\n",
    "    assert len(X) == len(target)\n",
    "    p = numpy.random.permutation(len(target))\n",
    "    X, target = X[p], target[p]\n",
    "    \n",
    "    X_train = X[:int(floor(len(X)*ratio)),:]\n",
    "    X_test = X[int(floor(len(X)*ratio)):,:]\n",
    "    \n",
    "    target_train = target[:int(floor(len(target)*ratio)),:]\n",
    "    target_test = target[int(floor(len(target)*ratio)):,:]\n",
    "    \n",
    "    return X_train, X_test, target_train, target_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(alpha, n, X, y, labda):\n",
    "    w = ones((shape(X)[1], 1))\n",
    "    for i in range(n):\n",
    "        w -= alpha * ((X.T@(X@w-y))/shape(y)[0] - 2*labda*w)\n",
    "    return w\n",
    "\n",
    "def cost(X, w, y, labda):\n",
    "    temp = X@w-y\n",
    "    temp = (temp.T@temp) / shape(X)[0] - labda*sqrt((w.T@w))\n",
    "    return temp[0,0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labda = 0.001 # magic number\n",
    "alpha = 0.0001 # magic numer\n",
    "iterations = 100000\n",
    "n = 2\n",
    "\n",
    "\n",
    "w_f, w_t, r_f, r_t = read_input_data()\n",
    "\n",
    "for j in range(5):\n",
    "    alpha *= 0.1\n",
    "    for i in range(1,n+1):\n",
    "        X = design_matrix(i, w_f, r_f)\n",
    "        target = append(w_t, r_t, axis=0)\n",
    "        X_train, X_test, target_train, target_test =  train_test_split(X, target, 0.90)\n",
    "\n",
    "        w = gradient_descent(alpha, iterations, X_train, target_train, labda)\n",
    "        wine_cost = cost(X_test, w, target_test, labda)\n",
    "        print(\"The cost of the test set is %s.\"%(wine_cost))\n",
    "        print(\"There are polynomials of degree %d in the design matrix and the learning rate is %s.\\n\"%(i,alpha) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of test results\n",
    "The wine dataset has been used to train several models with linear regression. We can see in the test results above that the design matrix that contains the second degree polynomials don't give a useful model since the regression does not converge untill the learning rate is lower than 1e-9. In comparison, the linear polyomials converges to a decent result when the learning rate is 1e4 times higher. This shows that using second degree polynomials causes the linear tegession odel to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens to the cost when we increase the regularization parameter?\n",
    "c = []\n",
    "labda_space = linspace(0.1, 0.4, 15)\n",
    "for l in labda_space:\n",
    "    w = gradient_descent(alpha, iterations, X_train, target_train, l)\n",
    "    c.append(cost(X_test, w, target_test, l))\n",
    "c = array(c)\n",
    "\n",
    "plt.plot(labda_space, c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_result(X, w, y, dimension=1):\n",
    "    plt.scatter(X[:, dimension], y, alpha=0.3)\n",
    "    plt.scatter(X[:, dimension], X@w, alpha=0.3)\n",
    "\n",
    "# Plot testdata with found weights for each dimension\n",
    "plt.figure(figsize=(15,20))\n",
    "for dim in range(12):\n",
    "    plt.subplot(6, 2, dim + 1)\n",
    "    plot_result(X_test, w, target_test, dimension=dim)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
