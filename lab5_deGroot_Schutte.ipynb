{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Principles in Pattern Recognition (2016/2017)\n",
    "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bA}{\\mathbf{A}}$\n",
    "$\\newcommand{\\bQ}{\\mathbf{Q}}$\n",
    "$\\newcommand{\\bR}{\\mathbf{R}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will work on one final project, instead of the step-by-step exercises of previous labs.\n",
    "\n",
    "## 1. Final project: Overfitting\n",
    "**[100 points]** Create a project about overfitting in the remainder of this notebook using markdown cells for equations and comments and code cells for code. Make sure to touch upon the following topics:\n",
    "1. Use the wine data set to show what *overfitting* is in terms of a regression problem. (see: white_data.npy, white_targets.npy, red_data.npy, red_targets.npy)\n",
    "2. Discuss how low and high *bias* and *variance* come into play here using figure(s), and write down what *model complexity* has to do with it.\n",
    "3. One way to deal with your overfitted data in a frequentist setting is regularized regression. Use your pick of regularized regression here and apply a cross-validation scheme to determine the regularization parameter $\\lambda$. \n",
    "4. Finally, shortly explain the Bayesian point-of-view on what you have done and how this would prevent overfitting. How could you use the Bayesian method to select the best model for your data? Contrast between model averaging and model selection and use the latter to select a good model.\n",
    "\n",
    "For more background information, refer to Bishop 1.1, 1.3, 1.5, 3.1.4, 3.2, 3.4!\n",
    " \n",
    "Notes on implementation:\n",
    "* Make sure that your hand-in is self-contained, understandable to read from start to end with an introduction about overfitting and overall conclusion or outlook.\n",
    "* This time we emphasize code cleanness and will allocate **[20 points]** to the readability of your code and graphical output.\n",
    "* Use your own implementations instead of standard Python machine-learning tools, like `sk-learn`. More standard modules like `numpy` are allowed as always.\n",
    "* As always: make sure you submit all included data and files necessary to run your notebook out-of-the-box!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting: Drunk On Regularized Regression\n",
    "### Lab 5\n",
    "Micha de Groot, 10434410  \n",
    "Jan Schutte, 11030844\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In this lab we aim to give insight into the problem of overfitting in regression models.\n",
    "Recall that minimizing the Sum of Squared Errors will yield the best fit for a given dataset:\n",
    "\n",
    "$$ \\sum_{i = 1}^n (y_i - f(w, x_i))^2 $$\n",
    "\n",
    "Where function $f$ gives us the prediction for input $x$ and weight vector $w$.\n",
    "\n",
    "This problem with this method is that we create a fit that exactly fits our dataset, if we then use our weight vector on another set of data we will see that the model predicts it poorly. This happens because this method creates a very complicated model that will go through almost all of the points of it's dataset, thus not taking into account the random noise we presume is on our data. In this lab we show how regularization might solve this problem by assigning a penalty to complex models.\n",
    "\n",
    "### Dataset\n",
    "The dataset used here descibes the quality of a red and white wine given 11 properties of that wine (fixed acidity,  volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, \n",
    "pH, sulphates, alcohol) with a target value of the quality score between 0 and 10.\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "Below is the boilerplate code for reading the dataset, creating the designmatrix and splitting the datasets into two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input_data():\n",
    "    \"\"\" Read inputdata files in Python 3 format.\"\"\"\n",
    "    white_feat = load('white_data_beter.npy')\n",
    "    white_tar = load('white_targets_beter.npy')\n",
    "    red_feat = load('red_data_beter.npy')\n",
    "    red_tar = load('red_targets_beter.npy')\n",
    "    return white_feat, white_tar, red_feat, red_tar\n",
    "\n",
    "\n",
    "def design_matrix(n, white_feat, red_feat):\n",
    "    \"\"\" Creates design matrix for white and red wine datasets.\n",
    "    n : degree of polynomials\n",
    "    white_feat: Input features for white dataset\n",
    "    red_feat: Input features for red dataset\n",
    "    \"\"\"\n",
    "    white_size = shape(white_feat)\n",
    "    red_size = shape(red_feat)\n",
    "    feat_size = 1 + n*white_size[1]\n",
    "    X = ones((white_size[0]+red_size[0], feat_size))\n",
    "    if n==1:\n",
    "        X[:white_size[0], 1:] = white_feat\n",
    "        X[white_size[0]:, 1:] = red_feat\n",
    "        return X\n",
    "    elif n== 2:\n",
    "        X[:white_size[0], 1:white_size[1]+1] = white_feat\n",
    "        X[white_size[0]:, 1:white_size[1]+1] = red_feat\n",
    "        X[:white_size[0], white_size[1]+1:] = square(white_feat)\n",
    "        X[white_size[0]:, white_size[1]+1:] = square(red_feat)\n",
    "        return X\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "def train_test_split(X, target, ratio):\n",
    "    \"\"\" Shuffle and split dataset into test set and training set.\"\"\"\n",
    "    assert len(X) == len(target)\n",
    "    p = numpy.random.permutation(len(target))\n",
    "    X, target = X[p], target[p]\n",
    "    \n",
    "    X_train = X[:int(floor(len(X)*ratio)),:]\n",
    "    X_test = X[int(floor(len(X)*ratio)):,:]\n",
    "    \n",
    "    target_train = target[:int(floor(len(target)*ratio)),:]\n",
    "    target_test = target[int(floor(len(target)*ratio)):,:]\n",
    "    \n",
    "    return X_train, X_test, target_train, target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "This is an iterative method for finding minimal value of the derivative of a function. Here we use it to find the lowest value of our cost function. The algorithm starts at some point in the parameter space, and it calculates the derivative of that point for each dimension of w. This will give the direction of the slope in that point, which will be the direction where we will choose the next point. By moving in the direction of the slope we will eventually fall into the 'valley' where the cost function is minimized. \n",
    "\n",
    "The alpha parameter is the learning rate, a scalar value that tells us how far away we choose the next point. Choosing this parameter is important, as a too small value will not yield results fast enough and too value that is too big will cause the algorithm to overshoot the 'valley' we are looking for.\n",
    "\n",
    "### Regularization\n",
    "Both the gradient descent and cost functions have a regularization parameter lamdba, this parameter inflates the values of the cost function by adding the square of the weight vector to the cost thus favoring a weight vector that is small in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(alpha, n, X, y, labda):\n",
    "    w = ones((shape(X)[1], 1))\n",
    "    for i in range(n):\n",
    "        w -= alpha * ((X.T@(X@w-y))/shape(y)[0] - 2*labda*w)\n",
    "    return w\n",
    "\n",
    "def cost(X, w, y, labda):\n",
    "    temp = X@w-y\n",
    "    temp = (temp.T@temp) / shape(X)[0] - labda*sqrt((w.T@w))\n",
    "    return temp[0,0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of test results\n",
    "The wine dataset has been used to train several models with linear regression. We can see in the test results below that the design matrix that contains the second degree polynomials don't give a useful model since the regression does not converge untill the learning rate is lower than 1e-9. In comparison, the linear polyomials converges to a decent result when the learning rate is 1e4 times higher. This shows that using second degree polynomials causes the linear regession model to overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labda = 0.001 # magic number\n",
    "alpha = 0.0001 # magic numer\n",
    "iterations = 100000\n",
    "n = 2\n",
    "\n",
    "\n",
    "w_f, w_t, r_f, r_t = read_input_data()\n",
    "\n",
    "for j in range(5):\n",
    "    alpha *= 0.1\n",
    "    for i in range(1,n+1):\n",
    "        X = design_matrix(i, w_f, r_f)\n",
    "        target = append(w_t, r_t, axis=0)\n",
    "        X_train, X_test, target_train, target_test = train_test_split(X, target, 0.90)\n",
    "\n",
    "        w = gradient_descent(alpha, iterations, X_train, target_train, labda)\n",
    "        wine_cost = cost(X_test, w, target_test, labda)\n",
    "        print(\"The cost of the test set is %s.\"%(wine_cost))\n",
    "        print(\"There are polynomials of degree %d in the design matrix and the learning rate is %s.\\n\"%(i,alpha) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the lambda parameter\n",
    "The regularization parameter we choose affects the penalty we assign to a more complex model. This parameter balances the model's complexity; we don't want a model that is too simple, but also don't want it too complicated as it will cause our model to overfit. What we have done below is train our model using the training set and calculating the cost on the test set while increasing the lambda from zero to one. \n",
    "\n",
    "As can be seen from the graph below, a low regularization parameter causes the model to underfit and results in a high cost on the test set. Then when the regularization parameter increases even more the cost again increases as we are overfitting our model to the trainset. We see for this model the cost is minimized for lambda $0.6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What happens to the cost when we increase the regularization parameter?\n",
    "X = design_matrix(1, w_f, r_f)\n",
    "target = append(w_t, r_t, axis=0)\n",
    "X_train, X_test, target_train, target_test = train_test_split(X, target, 0.90)\n",
    "alpha = 0.0001\n",
    "iterations = 1000\n",
    "c = []\n",
    "labda_space = linspace(0, 1.2, 100)\n",
    "for l in labda_space:\n",
    "    w = gradient_descent(alpha, iterations, X_train, target_train, l)\n",
    "    c.append(cost(X_test, w, target_test, l))\n",
    "c = array(c)\n",
    "\n",
    "plt.plot(labda_space, c);\n",
    "plt.ylabel(\"cost\")\n",
    "plt.xlabel(\"lambda\")\n",
    "\n",
    "# Print minimum lambda\n",
    "idx = np.argwhere(c == np.min(c))[0,0]\n",
    "found_lambda = labda_space[idx]\n",
    "print(\"Best lambda:%f\"% found_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting model predictions\n",
    "In the graphs below we have plotted the prediction of our model for the inputs of the test set. Each graph displays one of the features' dimension. In blue are the actual test set data and in orange our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_result(X, w, y, dimension=1):\n",
    "    plt.scatter(X[:, dimension], y, alpha=0.3)\n",
    "    plt.scatter(X[:, dimension], X@w, alpha=0.3)\n",
    "\n",
    "w = gradient_descent(alpha, iterations, X_train, target_train, found_lambda)\n",
    "    \n",
    "# Plot testdata with found weights for each dimension\n",
    "plt.figure(figsize=(15,20))\n",
    "for dim in range(12):\n",
    "    plt.subplot(6, 2, dim + 1)\n",
    "    plot_result(X_test, w, target_test, dimension=dim)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian approach\n",
    "\n",
    "The problem of overfitting can also be solved by using a bayesian approach, by assigning a prior distribution to the parameter space the model becomes more robust. The posterior distribution is now dependent on both the prior  and likelihood, as opposed to the frequentist approach where we only look at the likelihood. This also has the added benefit that small dataset can still result in good predictions given that a good prior distribution is chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
