{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Principles in Pattern Recognition (2017/2018)\n",
    "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bA}{\\mathbf{A}}$\n",
    "$\\newcommand{\\bQ}{\\mathbf{Q}}$\n",
    "$\\newcommand{\\bR}{\\mathbf{R}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "In the computer labs we will work with the Python programming language within a Jupyter notebook. Each week a new notebook is made available that contains the exercises that are to be handed-in. \n",
    "\n",
    "* You are expected to work in pairs.\n",
    "* Only one of each pair has to submit on blackboard. Make sure that you add the student ID of your partner in the submission comments.\n",
    "* The main notebook file you submit should read \"Lab[number]_[last name 1]_[last name 2].ipynb\", for example \"Lab2_Bongers_Versteeg.ipynb\". \n",
    "* Please make sure your code will run without problems!\n",
    "\n",
    "Feel free ask any questions during the computer lab sessions, or email the TA, Elise (e.e.vanderpol@uva.nl).\n",
    "\n",
    "**The due date for the labs is Friday, Sep 22 at 15:00**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Simple linear regression\n",
    "In this exercise, you will generate noisy data for a simple linear system $y = \\beta_0 + \\beta_1 x$ where we have one target and one predictor variable, and compute the solution of least-squares estimator using linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate data\n",
    "**[10 points]** Sample 20 datapoints $({x_i, y_i})$ for the following model: \n",
    "$$x \\in \\mathcal{N}(0, 3)$$\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon \\mathrm{,}$$\n",
    "where $\\epsilon \\in \\mathcal{N}(0,1)$ is standard normal and the parameters $\\beta_0$ and $\\beta_1$ are both taken randomly __once__ in the interval $(0,1)$ (and are thus kept fixed during sampling!). Make a scatterplot of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = np.vectorize(np.random.normal)\n",
    "data_size = 20\n",
    "beta0 = np.random.random()\n",
    "beta1 = np.random.random()\n",
    "\n",
    "dataX = func(np.zeros(data_size),3)\n",
    "noise = func(np.zeros(data_size),1)\n",
    "dataY = (dataX*beta1) + beta0 + noise\n",
    "\n",
    "plt.scatter(dataX,dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  LS solution\n",
    "**[15 points]** What is the solution of the least-squares estimate for the parameters $\\hat{\\bb} = (\\hat{\\beta}_0, \\hat{\\beta}_1)$ for model $y = \\beta_0 + \\beta_1 x$ in terms of the dependent variable $y$ and the design matrix $\\mathbf{X}$? Write the solutions to these normal equations down in closed form in the markdown cell below. What are the dimensions of $\\hat{\\bb}$, $\\bX$ and $\\by$? \n",
    "\n",
    "Hint: see Bishop chapter 3, it helps to write the model in vectorized form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{b} = (X^t X)^{-1}X^T y $$\n",
    "\n",
    "Dimensions:  \n",
    "$\\hat{b}$: 2x1  \n",
    "$X$: 20x2  \n",
    "$y$: 20x1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compute estimates\n",
    "**[10 points]** Create the design matrix $\\bX$ and compute the point estimation for the parameters $\\hat{\\bb}$ using the simulated data from 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrix([np.ones(data_size), dataX]).T\n",
    "bHat = inv((X.T * X)) * X.T * matrix(dataY).T\n",
    "bHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Plotting\n",
    "**[5 points]** Plot the model with the estimated parameters and the true parameters in the same scatterplot as the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dataX,dataY)\n",
    "xPlot = linspace(amin(X), amax(X), 30)\n",
    "plt.plot(xPlot, (bHat[0] + bHat[1]*xPlot).T)\n",
    "plt.plot(xPlot, (beta0 + beta1*xPlot).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Multiple linear regression\n",
    "Here we go from simple to multiple linear regression, and encounter some difficulties that may arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Repeating for multiple regression\n",
    "**[30]** Sample now 20 datapoints from a more complex model with multiple prediction variables:\n",
    "$$x_1 \\in \\mathcal{N}(0, 3)$$\n",
    "$$x_2 \\in \\mathcal{N}(0, 2)$$\n",
    "$$x_3 \\in \\mathcal{N}(0, 3)$$\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon \\mathrm{,}$$\n",
    "where $\\epsilon$ is again standard normal and the $\\beta_i$ are all randomly set in $(0,1)$. Repeat the above assignment by computing $\\bX$ and the least squares estimate of the parameters $\\hat{\\bb}$. Print the results for  $\\hat{\\bb}$ and the original parameters $\\bb$ together as a check.\n",
    "\n",
    "Hint: if you vectorized the computation in 1.3 correctly, you could re-use some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 20\n",
    "\n",
    "beta = matrix(random.random(size=4)).T\n",
    "\n",
    "X = ones(shape=(4, data_size))\n",
    "X[1] = func(np.zeros(data_size),3)\n",
    "X[2] = func(np.zeros(data_size),2)\n",
    "X[3] = func(np.zeros(data_size),3)\n",
    "X = matrix(X.T)\n",
    "noise = matrix(func(np.zeros(data_size),1)).T\n",
    "\n",
    "Y = X*beta + noise\n",
    "bHat = inv((X.T * X)) * X.T * Y\n",
    "#print(bHat)\n",
    "#print(beta)\n",
    "print(\"Difference between bHat and the original beta: \\n\", bHat-beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded some multivariate data, comprising of one target $y$ and 3 predictor variables $x_i$ as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## example data.\n",
    "data = np.load('data.npz')\n",
    "x_example = matrix(data['x'])\n",
    "y_example = matrix(data['y']).T\n",
    "designmatrix_example = data['designmatrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Example data\n",
    "**[5 points]** Use the designmatrix $\\bX$ and target variable $\\by$ from the data loaded above in your implementation from problem 2 to get a least-square estimate for $\\bb$ assuming a linear regression model. What happens, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrix(designmatrix_example)\n",
    "try:\n",
    "    bHat = inv((X.T * X)) * X.T * y_example\n",
    "except:\n",
    "    print('Determinant is:', det(X.T * X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens, and why?\n",
    "In the example x data we can see the first and third column have the same values, meaning that they are dependent and when reduced the matrix really has rank 2. This implies the design matrix really has rank 3, and cannot be inverted. This usually does not happen in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can factor any complex (and thus real-valued) $m \\times n$ matrix $\\bA$ in terms of an $m \\times n$ orthogonal matrix $\\bQ$ and an $n \\times n$ upper triangular matrix $\\bR$, a so-called QR decomposition.  \n",
    "### 2.3 QR decomposition\n",
    "**[10 points]** Rewrite the solution to the normal equations from 1.2 in terms of the QR decomposition of the designmatrix. Why could this form be more preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{align}\n",
    "&\\hat{b} = ((QR)^T QR)^{-1}(QR)^T y  \\\\\n",
    "&= (R^TQ^T QR)^{-1}(QR)^T y  \\\\\n",
    "&= (R^T R)^{-1}R^T Q^T y \\\\\n",
    "&= R^{-1}(R^T)^{-1} R^T Q^T y \\\\\n",
    "&= R^{-1} Q^T y \\\\\n",
    "\\end{align}\n",
    "\n",
    "Inverse van R bestaat niet want R is niet vierkant.  \n",
    "Ook doen we de WR decompositie verkeerd want we maken R wel vierkant en Q niet.  \n",
    "\n",
    "Computing the estimated parameters using the QR decomposition is computationally much easier because computing the inverse of the uppper triangular matrix is faster. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Implementation\n",
    "There are many different methods to calculate the QR decomposition to find here we will use a modified Gram-Schmidt orthogonalization algorithm as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modified_gram_schmidt(mat):\n",
    "    mat = mat.copy()\n",
    "    m, n = mat.shape\n",
    "    \n",
    "    q = np.zeros((m,n))\n",
    "    r = np.zeros((n,n))\n",
    "\n",
    "    for k in range(n):\n",
    "        # Normalization coeffients of the columns \n",
    "        r[k, k]     = np.linalg.norm(mat[:, k])\n",
    "        # Apply normalization\n",
    "        q[:, k]     = mat[:,k]/r[k,k]\n",
    "        # Compute the projection on the remaining columns\n",
    "        r[k, k+1:n] = np.dot(q[:,k], mat[:,k+1:n])\n",
    "        # subtract the projection to get orthogonal columns\n",
    "        mat[:, k+1:n] = mat[:,k+1:n] - np.outer(q[:,k], r[k,k+1:n])\n",
    "        \n",
    "    print(matrix(r).T*matrix(r))\n",
    "    return matrix(q), matrix(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10 points]** Explain __roughly__ why this algorithm would return an orthogonalized $\\bQ$ (you are allowed to add comments to the code above). Apply it on the designmatrix of your example, show that the decomposition works and calculate an estimate $\\hat{\\bb}$.\n",
    "\n",
    "Each next column of Q is made by subtracting the projection of the original column in the matrix\n",
    "from the column. What remains is the part of the column that is orthogonal to the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, r = modified_gram_schmidt(array(X))\n",
    "bHat = inv(r)*q.T*y_example\n",
    "bHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Numpy\n",
    "**[5 points]**  Use the least-squares implementation in _numpy_ on the example data to get the parameters for $\\by = \\bX \\bb$  and compare with what you found in 2.2 and 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bHat2 = lstsq(X, y_example)[0]\n",
    "bHat2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
