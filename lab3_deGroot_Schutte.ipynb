{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Principles in Pattern Recognition (2017/2018)\n",
    "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
    "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
    "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bA}{\\mathbf{A}}$\n",
    "$\\newcommand{\\bQ}{\\mathbf{Q}}$\n",
    "$\\newcommand{\\bR}{\\mathbf{R}}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol{\\pi}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "In the computer labs we will work with the Python programming language within a Jupyter notebook. Each week a new notebook is made available that contains the exercises that are to be handed-in. \n",
    "\n",
    "* It is expected that you work in pairs but individual submissions are allowed.\n",
    "* Only one of each pair has to submit on blackboard. Make sure that you add the student ID of your partner in the submission comments.\n",
    "* The main notebook file you submit should read \"Lab[number]_[last name 1]_[last name 2].ipynb\", for example \"Lab2_Bongers_Versteeg.ipynb\". \n",
    "* Please make sure your code will run without problems!\n",
    "\n",
    "Feel free ask any questions during the computer lab sessions, or email the TA, Elise (e.e.vanderpol@uva.nl).\n",
    "\n",
    "**The due date for the labs is Friday, Sep 29 at 15:00**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Maximum likelihood estimation\n",
    "In this exercise, our goal is to use maximum likelihood estimation with gradient descent to estimate the probability density by using of a set of  samples drawn from a multivariate Gaussian. \n",
    "\n",
    "Note: even though there exists a close form solution for this case, we will go the academic route and use numerical optimization anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multivariate Gaussian\n",
    "**[5 points]** Draw 250 samples from a multivariate Gaussian with parameters\n",
    "$$\n",
    "\\bmu =\n",
    "\\begin{pmatrix}\n",
    "        0.3 \\\\\n",
    "        -1.8 \\\\\n",
    "\\end{pmatrix}\\mathrm{;\\ }\n",
    "\\bSigma =\n",
    "\\begin{pmatrix}\n",
    "        2.1 & 1.3 \\\\\n",
    "        1.3 & 4.9 \\\\\n",
    "\\end{pmatrix}\\mathrm{;\\ }\n",
    "$$\n",
    "and make a scatterplot of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mu = array([0.3, -1.8])\n",
    "sigma = array([[2.1, 1.3], [1.3, 4.9]])\n",
    "\n",
    "data = multivariate_normal(mu, sigma, size=250)\n",
    "# print(data)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Log likelihood\n",
    "**[15 points]** Write down the log likelihood of this two-dimensional Gaussian model and write a function `loglik(data, mu, sigma)` that returns the log likelihood for the data that it is given. (We will use this later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik(data, mu, sigma):\n",
    "    result = log(1.0 / sqrt(2*pi ** len(mu)))\n",
    "    result += log(1.0 / sqrt(det(sigma)))\n",
    "    sigma_inv = inv(sigma)\n",
    "    for i in range(len(data)):\n",
    "        temp = -1.0/2.0*(data[i]-mu).T@sigma_inv@(data[i]-mu)\n",
    "        result += temp\n",
    "    return result\n",
    "\n",
    "\n",
    "loglik(data, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gradients\n",
    "**[15 points]** Write down the gradient of the log likelihood with respect to $\\boldsymbol{\\mu}$, we already provided the gradient w.r.t. $\\bSigma$ below:\n",
    "$$\n",
    "\\nabla_{\\bSigma} (\\log \\mathcal{L}) = - \\frac{n}{2} \\bSigma^{-1} + \\frac{n}{2} \\bSigma^{-1} \\boldsymbol{S} \\bSigma^{-1}\\mathrm{,}\n",
    "$$\n",
    "where $\\boldsymbol{S} = \\frac{1}{n} \\sum_i^n (\\bx_i- \\bmu)(\\bx_i- \\bmu)^T$. (if you like to derive it as an exercise, feel free to do so.)\n",
    "\n",
    "Write a function `grad_loglik(x, mu, sigma)` that returns the gradients as evaluated on the data set `x`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loglik(x, mu, sigma):\n",
    "    grad_sigma = 0\n",
    "    grad_mu = 0\n",
    "    for vec in x:\n",
    "        grad_mu += vec-mu\n",
    "        grad_sigma += matrix(vec-mu).T@matrix(vec-mu)\n",
    "    grad_mu = -inv(sigma) @ grad_mu\n",
    "    grad_sigma /= len(x)\n",
    "    grad_sigma = -len(x)/2*inv(sigma) + len(x)/2*inv(sigma)@grad_sigma@inv(sigma)\n",
    "    return (grad_mu, grad_sigma)\n",
    "    \n",
    "grad_loglik(data, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Numerical optimization\n",
    "**[35 points]** Numerically compute maximum likelihood estimates of all parameters. Implement the gradient ascent (or descent, https://en.wikipedia.org/wiki/Gradient_descent) numerical optimization method, by using the functions you created in 1.2 and 1.3. I.e. after initializing all parameters to reasonable values, repeat:\n",
    "* calculate gradients for the data \n",
    "* update the parameters with the gradients and an appropriate learning rate\n",
    "\n",
    "Keep track of the log likelihood after each step and make a plot of this. \n",
    "Run it for several iterations and visualize the resulting distribution with the data in the same figures (either 2 1D projections or a fancy 2D plot is accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nonparametric density estimation\n",
    "Up to now we have used parametric models for (density) estimation. One example of a nonparametric method we used earlier is the (normalized) histogram method, with fixed binning. We will explore an example of a  nonparametric method: kernel density estimation.\n",
    "\n",
    "Consider the data loaded below, which is drawn from a mixture of 3 Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "with open('mixture.pkl', 'r') as f:\n",
    "    mixed_data = cPickle.load(f)\n",
    "_ = plt.hist(mixed_data, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Histograms\n",
    "\n",
    "**[10 points]** Using the `mixed_data` above, illustrate two potential disadvantages of the histogram method as a nonparametric method for density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Kernel density approximation\n",
    "**[20 points]** One other way to estimate the density given some data sample, is by using kernels. A kernel $f_h(x)$ is a function of the data, given some smoothing parameter $h$, that specifies the local neighbourhood around a point. (See Bishop 2.5.2 or Hastie 2.8.2 or Google). \n",
    "\n",
    "Specify pros and cons of KDE over the histogram method by creating plots of the `mixed_data` above. Use in your arguments one or two different kernels with one or two different bandwidths. You are allowed to use a KDE code from whatever python package you prefer, for example `sklearn` or `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
